---
title: "Week 10 Assignment"
author: "Atina Karim"
date: "10/29/2020"
output: 
html_document:
  toc: true
  toc_float: true
  toc_depth: 3
  df_print: paged
  theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# *ASSIGNMENT*
In Text Mining with R, Chapter 2 looks at Sentiment Analysis.  In this assignment, you should start by getting the primary example code from chapter 2 working in an R Markdown document.  You should provide a citation to this base code.  You’re then asked to extend the code in two ways:

Work with a different corpus of your choosing, and
Incorporate at least one additional sentiment lexicon (possibly from another R package that you’ve found through research).
As usual, please submit links to both an .Rmd file posted in your GitHub repository and to your code on rpubs.com.  

# Sentiment analysis with tidy data

The code chunks and texts below are from [Chapter 2 of Text Mining with R](https://www.tidytextmining.com/sentiment.html) (Silge and Robinson, 2020)

First, we will load the required libraries and take a look at the different sentiment lexicons.

```{r,  message=FALSE}
library(janeaustenr)
library(tidyverse)
library(stringr)
library(tidytext)
library(jsonlite)
library(dplyr)
library (ggplot2)
```



```{r, interactive=TRUE}
get_sentiments("afinn")
```

.
```{r}
get_sentiments("bing")
```

```{r}
get_sentiments("nrc")
```
## Sentiment analysis with inner join
Let’s look at the words with a joy score from the NRC lexicon. What are the most common joy words in Emma? 
```{r}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
      ignore_case = TRUE
    )))
  ) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```

Now that the text is in a tidy format with one word per row, we are ready to do the sentiment analysis. First, let’s use the NRC lexicon and filter() for the joy words. Next, let’s filter() the data frame with the text from the books for the words from Emma and then use inner_join() to perform the sentiment analysis. What are the most common joy words in Emma?
```{r}
nrc_joy <- get_sentiments("nrc") %>%
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

Next, we count up how many positive and negative words there are in defined sections of each book. We define an index here to keep track of where we are in the narrative; this index (using integer division) counts up sections of 80 lines of text.
Small sections of text may not have enough words in them to get a good estimate of sentiment while really large sections can wash out narrative structure. For these books, using 80 lines works well, but this can vary depending on individual texts, how long the lines were to start with, etc. We then use spread() so that we have negative and positive sentiment in separate columns, and lastly calculate a net sentiment (positive - negative).

```{r, message=FALSE}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```
Now we can plot these sentiment scores across the plot trajectory of each novel.
```{r}
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```
## Comparing the three sentiment dictionaries
With several options for sentiment lexicons, you might want some more information on which one is appropriate for your purposes. Let’s use all three sentiment lexicons and examine how the sentiment changes across the narrative arc of Pride and Prejudice. 
```{r}
pride_prejudice <- tidy_books %>%
  filter(book == "Pride & Prejudice")

pride_prejudice
```
```{r, message=FALSE}
afinn <- pride_prejudice %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(index = linenumber %/% 80) %>%
  summarise(sentiment = sum(value)) %>%
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>%
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>%
    inner_join(get_sentiments("nrc") %>%
      filter(sentiment %in% c(
        "positive",
        "negative"
      ))) %>%
    mutate(method = "NRC")
) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```
We now have an estimate of the net sentiment (positive - negative) in each chunk of the novel text for each sentiment lexicon. Let’s bind them together and visualize them.
```{r}
bind_rows(
  afinn,
  bing_and_nrc
) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
Why is the result for the NRC lexicon biased so high in sentiment compared to the Bing et al. result? Let’s look briefly at how many positive and negative words are in these lexicons.
```{r}
get_sentiments("nrc") %>%
  filter(sentiment %in% c(
    "positive",
    "negative"
  )) %>%
  count(sentiment)

get_sentiments("bing") %>%
  count(sentiment)
```
## Most common positive and negative words
```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```
```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(
    y = "Contribution to sentiment",
    x = NULL
  ) +
  coord_flip()
```
```{r}
custom_stop_words <- bind_rows(
  tibble(
    word = c("miss"),
    lexicon = c("custom")
  ),
  stop_words
)

custom_stop_words
```
## Wordclouds

Let’s look at the most common words in Jane Austen’s works as a whole.
```{r}
library(wordcloud)

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```
Let’s do the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words. Until the step where we need to send the data to comparison.cloud(), this can all be done with joins, piping, and dplyr because our data is in tidy format.

```{r}
library(reshape2)

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(
    colors = c("gray20", "gray80"),
    max.words = 100
  )
```
## Looking at units beyond just words

We may want to tokenize text into sentences, and it makes sense to use a new name for the output column in such a case.
```{r}
PandP_sentences <- tibble(text = prideprejudice) %>%
  unnest_tokens(sentence, text, token = "sentences")
```
```{r}
PandP_sentences$sentence[2]
```

```{r}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text,
    token = "regex",
    pattern = "Chapter|CHAPTER [\\dIVXLC]"
  ) %>%
  ungroup()
# unnest splits into tokens using a regex pattern
austen_chapters %>%
  group_by(book) %>%
  summarise(chapters = n())
```

Let’s find the number of negative words in each chapter and divide by the total words in each chapter. For each book, which chapter has the highest proportion of negative words?

```{r}
bingnegative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords / words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()
```

# Work with a different corpus of your choosing: NYT Movie 

I would like to extend my [assignment from Week 9](https://rpubs.com/Atinakarim/680427), for which I looked at movie reviews for movies that were released in 2019. For this assignment, I will perform sentiment analysis on the summary of the NYT movie review, for movies released in 2019. 

## Fetching data from API
```{r}
url <- "https://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2019-01-01;2020-01-01"
key <- "OkVf8SLjqbsAbQAvVbiJBn6yRY7azROI"
addurl <- paste0(url, "&api-key=")
# fetched using json + key call
data <- fromJSON(paste0(addurl, key))
df <- data$results
knitr:: kable (df)
```

### Sentimentr
We will use the sentimentr package to try to understand the sentiments conveyed in the reviews as a whole.
The Sentimentr package allows the users to quickly perform sentiment analysis on sentences and it corrects for inversions. It assigns a score from -1 to 1 that indicates whether the sentiment is negative, neutral or positive.
```{r}
library(sentimentr)
library(data.table)
sentiment <- sentiment_by(df$summary_short)
View(sentiment)
```
The first column (element_id) in our case are the movies as they appear in the table above. Word_Count is the number of words in each sentences. The sentimentr package looks at each sentence in the review separately and calculates the overall average score and the standard deviation for the reviews. Most of the reviews in our case were one sentence long which is why our sd column is mostly empty.

#### Summarizing the sentiments

I want to convert the average sentiment scores into the following categories: positive, neutral and negative.

```{r}
#function that generates a sentiment class based on average score
sentiment_df<- setDF(sentiment)
get_sentiment_class <- function(ave_sentiment){
  sentiment_class="Positive"
if (ave_sentiment < -.3){
  sentiment_class = "Negative"}
else if (ave_sentiment<.3){
  sentiment_class = "Neutral"
}
sentiment_class
}
```

```{r}
sentiment_df$ave_sentiment <- 
  sapply(sentiment_df$ave_sentiment,get_sentiment_class)
sentiment_df
```
```{r}
ggplot(data=sentiment_df,aes(x=ave_sentiment,fill=ave_sentiment))+geom_bar()
```
It seems like most reviews were neutral. However, it is also interesting to see that there were more negative reviews than positive ones.

### Afinn

Let's see if we see similar results with the 'Afinn' lexicon:

```{r}
x <- tibble (txt=df$summary_short)
x <-x %>% unnest_tokens(word,txt)

```
```{r}
library(plyr)
y <-join(x,get_sentiments("afinn"),type="inner")
y
```
```{r}
y_df<- setDF(y)
get_sentiment_class <- function(value){
  sentiment_class="Positive"
if (value < (-3)){
  sentiment_class = "Negative"}
else if (value < (3)){
  sentiment_class = "Neutral"
}
sentiment_class
}
```
```{r}
y_df$value <- 
  sapply(y_df$value,get_sentiment_class)
y_df
```
```{r}
ggplot(data=y_df,aes(x=value,fill=value))+geom_bar()
```
Even with Afinn, we are seeing more neutral reviews. However, unlike with Sentimentr, Afinn did not detect any negative reviews.

## Which lexicon did you have was most useful for your corpus and why?

I thought the Sentimentr package was more useful for my corpus since it evaluates the entire sentence and is thus able to account for the context in which a word is being used.

Moreover,with Afinn,it seems like we are only limited to the list of words the lexicon contains. It is also interesting to see that Afinn gave the word 'ominous' a positive value.

**References**
Robinson, Julia Silge and David. “Text Mining with R.” 2 Sentiment Analysis with Tidy Data, 29 Oct. 2020, [www.tidytextmining.com/sentiment.html.](www.tidytextmining.com/sentiment.html. ) 

